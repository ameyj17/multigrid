mode: "ppo"
version: "1.0"

# PPO specific parameters optimized for SPEED
ppo_epochs: 8              # Optimal for learning
clip_param: 0.2            # Standard PPO clipping
value_loss_coef: 0.5       # Standard value
entropy_coef: 0.05         # Balance exploration vs exploitation
max_grad_norm: 0.5         # Standard gradient clipping 
gae_lambda: 0.95           # Good for credit assignment
gamma: 0.99                # Standard discount

# CRITICAL SPEED IMPROVEMENTS
batch_size: 256            # Larger batch size for parallel processing
learning_rate: 1.0e-3      # Much higher learning rate for faster convergence
num_steps: 256             # Shorter rollouts for more frequent updates
n_envs: 16                 # Number of parallel environments
buffer_size: 4096          # Larger replay buffer for parallel environments
target_kl: 0.03
# Network architecture - simplified
hidden_size: 64            # Smaller networks for faster training
activation: "relu"         # Fast activation function

# Minimal logging - critical for speed
save_interval: 100      # Only save once
log_interval: 1000         # Very infrequent logging

# Disable all debug output
verbose: 0